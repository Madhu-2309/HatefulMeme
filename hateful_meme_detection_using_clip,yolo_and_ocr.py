# -*- coding: utf-8 -*-
"""Hateful Meme Detection using clip,yolo and OCR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZphawnIV0n-Dvy1vFonoytMnhQc58Gd7
"""

!pip install torch torchvision transformers pandas scikit-learn tqdm

from google.colab import drive
drive.mount('/content/drive')

!pip install datasets transformers torch

!pip install datasets

import kagglehub

# Download latest version
path = kagglehub.dataset_download("parthplc/facebook-hateful-meme-dataset")

print("Path to dataset files:", path)

import os

print("Dataset Path: ", path)
print("Contents of the folder: ", os.listdir(path))

import os

data_path = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data"
print("Contents of data folder: ", os.listdir(data_path))

import os
image_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/img"
print("Contents of image folder: ", os.listdir(image_folder))

correct_image_path = os.path.join(image_folder, "42953.png")
print("Corrected image path: ", correct_image_path)

from PIL import Image
import IPython.display as display
# Open and display the image
img = Image.open(correct_image_path)
display.display(img)

!pip install torch torchvision torchaudio
!pip install openai-clip
!pip install ultralytics
!pip install pytesseract opencv-python
!pip install transformers datasets

import torch
import torchvision.transforms as transforms
from PIL import Image
import matplotlib.pyplot as plt
from datasets import load_dataset
import clip
import pytesseract
import cv2
from ultralytics import YOLO

# Set the dataset path from Kaggle download
dataset_path = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/"
# Load dataset from Hugging Face (already downloaded in-memory version)
dataset = load_dataset("emily49/hateful-memes")
# Check a sample from the training set
sample = dataset['train'][0]
print(sample)
# Construct the full image path
image_path = dataset_path + sample['img']
# Display the sample image
img = Image.open(image_path)
plt.imshow(img)
plt.axis('off')
plt.show()

# Construct the correct image path
image_path = dataset_path + sample['img']
print("Image Path: ", image_path)
# Verify if the image exists
import os
if os.path.exists(image_path):
    print("Image found!")
else:
    print("Image not found!")

import random
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# Convert dataset to a list of dictionaries
train_samples = list(dataset['train'])
# Select 10 random samples from the dataset
samples = random.sample(train_samples, 10)
# Set the image folder path (adjust if needed)
image_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/"
# Display the images with their captions and labels
plt.figure(figsize=(20, 20))
for i, sample in enumerate(samples):
    image_path = os.path.join(image_folder, sample['img'])
    img = mpimg.imread(image_path)
    plt.subplot(5, 2, i + 1)
    plt.imshow(img)
    plt.title(f"Text: {sample['text']}\nLabel: {sample['label']}")
    plt.axis('off')
plt.show()

!pip install easyocr

import torch
import clip
from PIL import Image
import easyocr
# Load CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model, preprocess = clip.load("ViT-B/32", device=device)
# Load OCR model
ocr_reader = easyocr.Reader(['en'])  # Supports English language
print("Models loaded successfully!")

import os
# Path to the dataset folder
image_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data"
# List directories and files to find the correct image path
for root, dirs, files in os.walk(image_folder):
    print(f"Directory: {root}")
    for file in files[:5]:  # Show first 5 files in each directory
        print(f"File: {file}")

image_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/img"

ls -R /root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/

img_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/img/"

import os

img_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/img/"
print("Total images found:", len(os.listdir(img_folder)))
print("Sample images:", os.listdir(img_folder)[:5])

from PIL import Image
import matplotlib.pyplot as plt

img_path = os.path.join(img_folder, os.listdir(img_folder)[0])
img = Image.open(img_path)

plt.imshow(img)
plt.axis("off")
plt.show()

import torch
from transformers import CLIPProcessor, CLIPModel
from PIL import Image
import os
import numpy as np

image_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/img"
annotation_path = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/train.jsonl"

print("Sample images:", os.listdir(image_folder)[:5])

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

import os

image_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/img"
image_files = os.listdir(image_folder)

print(f"Total images found: {len(image_files)}")
print("Sample images:", image_files[:10])

import json
annotation_path = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/train.jsonl"
with open(annotation_path, "r") as f:
    data = [json.loads(line) for line in f]
print(f"Total entries in annotation: {len(data)}")
print("Sample annotation entries:", data[:5])

# Extract image names from the annotation
annotation_images = [item["img"] for item in data]
image_folder_files = set(os.listdir(image_folder))
# Identify missing images
missing_images = [img for img in annotation_images if img not in image_folder_files]
print(f"Total missing images: {len(missing_images)}")
print("Sample missing images:", missing_images[:10])

import os
image_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/img"
print("Folder structure:\n")
!ls -R $image_folder

!kaggle datasets download -d parthplc/facebook-hateful-meme-dataset --unzip --force

import os

image_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/img"
print(f"Total images found: {len(os.listdir(image_folder))}")

!ls -R /root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/

annotation_images = [item["img"].split('/')[-1] for item in data]
image_folder_files = set(os.listdir(image_folder))
missing_images = [img for img in annotation_images if img not in image_folder_files]
print(f"Total missing images: {len(missing_images)}")

import torch
import clip
from PIL import Image
import os

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

import pandas as pd
import json
# Path to dataset
dataset_path = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/train.jsonl"
# Load JSONL file
data = [json.loads(line) for line in open(dataset_path, "r")]
# Display sample
print("Total samples:", len(data))
print(data[:3])  # Check structure

# Create a smaller subset (e.g., 100 samples)
data_subset = data[:1000]
print("Subset size:", len(data_subset))

import os
image_folder = "/root/.cache/kagglehub/datasets/parthplc/facebook-hateful-meme-dataset/versions/1/data/img"
# Check for missing images
missing_images = [item["img"] for item in data_subset if not os.path.isfile(os.path.join(image_folder, item["img"].split("/")[-1]))]
print(f"Total missing images: {len(missing_images)}")
print("Sample missing images:", missing_images[:10])

import torch
import clip
from PIL import Image
# Load CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)
# Prepare to store features
image_features = []
text_features = []
# Extract features
for item in data_subset:
    image_path = os.path.join(image_folder, item["img"].split("/")[-1])
    text = item["text"]
    if not os.path.isfile(image_path):
        print(f"Missing image: {image_path}")
        continue
    # Process image
    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)
    # Extract image and text features
    with torch.no_grad():
        image_feature = model.encode_image(image)
        text_feature = model.encode_text(clip.tokenize([text]).to(device))
    image_features.append(image_feature.cpu())
    text_features.append(text_feature.cpu())
# Combine features
image_features = torch.cat(image_features)
text_features = torch.cat(text_features)
print("Image Features Shape:", image_features.shape)
print("Text Features Shape:", text_features.shape)

import pickle

# Save features and labels
with open("features_subset.pkl", "wb") as f:
    pickle.dump({"image_features": image_features, "text_features": text_features, "labels": [item["label"] for item in data_subset]}, f)

print("Features saved successfully!")

print("Sample data item:", data_subset[:5])

labels = [item["label"] for item in data_subset]

print("Labels Sample:", labels[:10])
print("Total Labels:", len(labels))

import torch.nn as nn
class MultimodalClassifier(nn.Module):
    def __init__(self, input_dim=1024, hidden_dim=512, output_dim=2):
        super(MultimodalClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.softmax = nn.Softmax(dim=1)
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return self.softmax(x)
model = MultimodalClassifier()
print(model)

print("Image Features Shape:", image_features.shape)
print("Text Features Shape:", text_features.shape)

combined_features = torch.cat((image_features, text_features), dim=1)
print("Combined Feature Shape:", combined_features.shape)  # Should be [100, 1024]

print("Labels Shape:", torch.tensor(labels).shape)  # Should be [100]
print("Labels Sample:", labels[:10])  # Check a few sample labels

import torch.optim as optim
# Set up the model, loss, and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
# Move model and data to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)
# Prepare data for training
X = combined_features.to(device)  # Feature vectors
y = torch.tensor(labels, dtype=torch.long).to(device)  # Ensure labels are LongTensor

# Ensure both model and data are on the same device and dtype
X = X.to(device).float()  # Ensure input is float32
y = y.to(device).long()   # Ensure labels are LongTensor for CrossEntropyLoss
model = model.to(device).float()  # Ensure model is float32
# Training parameters
num_epochs = 10
batch_size = 16
# Dataset and DataLoader
from torch.utils.data import TensorDataset, DataLoader
dataset = TensorDataset(X, y)
train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
# Training loop
for epoch in range(num_epochs):
    model.train()  # Set model to training mode
    total_loss = 0
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device).float(), batch_y.to(device).long()
        optimizer.zero_grad()  # Clear previous gradients
        # Forward pass
        outputs = model(batch_X)
        # Compute loss
        loss = criterion(outputs, batch_y)
        # Backward pass and optimization
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss:.4f}")
print("Training Completed!")

# Evaluation mode
model.eval()
correct = 0
total = 0
with torch.no_grad():  # Disable gradient computation for evaluation
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device).float(), batch_y.to(device).long()
        # Forward pass
        outputs = model(batch_X)
        # Get predicted class (highest logit)
        _, predicted = torch.max(outputs, dim=1)
        total += batch_y.size(0)
        correct += (predicted == batch_y).sum().item()
accuracy = correct / total * 100
print(f"Training Accuracy: {accuracy:.2f}%")

# Save model to file
torch.save(model.state_dict(), "multimodal_hateful_memes_model.pth")
print("Model saved successfully!")

for i in range(5):
    print(f"Text: {data_subset[i]['text']}")
    print(f"Actual Label: {labels[i]}")
    output = model(combined_features[i].unsqueeze(0).to(device).float())
    predicted_label = torch.argmax(output, dim=1).item()
    print(f"Predicted Label: {predicted_label}")
    print("===")

from sklearn.model_selection import train_test_split

# Split combined features and labels (80% train, 20% validation)
X_train, X_val, y_train, y_val = train_test_split(combined_features, labels, test_size=0.2, random_state=42)

# Move to GPU if available
X_train, X_val = X_train.to(device), X_val.to(device)
y_train, y_val = torch.tensor(y_train).to(device), torch.tensor(y_val).to(device)

print(f"Training Samples: {len(X_train)}, Validation Samples: {len(X_val)}")

# Ensure both features and labels are float32
X_train = X_train.float()
X_val = X_val.float()

# Ensure the model is also using float32
model = model.to(device).float()

model.eval()

correct = 0
total = 0

with torch.no_grad():
    outputs = model(X_val)
    _, predicted = torch.max(outputs, dim=1)

    total = y_val.size(0)
    correct = (predicted == y_val).sum().item()

val_accuracy = correct / total * 100
print(f"Validation Accuracy: {val_accuracy:.2f}%")

# Check for overlap between training and validation sets
overlap = set(y_train.cpu().numpy()).intersection(set(y_val.cpu().numpy()))
print(f"Overlapping samples: {len(overlap)}")

# Use unseen data for evaluation (e.g., last 10 samples)
X_test = combined_features[-10:].float().to(device)
y_test = torch.tensor(labels[-10:]).to(device)

# Predict on unseen samples
with torch.no_grad():
    outputs = model(X_test)
    _, predicted = torch.max(outputs, dim=1)

print("Model Predictions on Unseen Data:")
for i in range(10):
    print(f"Text: {data_subset[-10:][i]['text']}")
    print(f"Actual Label: {y_test[i].item()}, Predicted Label: {predicted[i].item()}")
    print("=" * 50)

# Use unseen data for evaluation (e.g., last 10 samples)
X_test = combined_features[-10:].float().to(device)
y_test = torch.tensor(labels[-10:]).to(device)

# Predict on unseen samples
with torch.no_grad():
    outputs = model(X_test)
    _, predicted = torch.max(outputs, dim=1)

print("Model Predictions on Unseen Data:")
for i in range(10):
    print(f"Text: {data_subset[-10:][i]['text']}")
    print(f"Actual Label: {y_test[i].item()}, Predicted Label: {predicted[i].item()}")
    print("=" * 50)

optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

import torch.optim as optim

# Updated optimizer with weight decay for L2 regularization
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

num_epochs = 10

for epoch in range(num_epochs):
    model.train()  # Set model to training mode
    optimizer.zero_grad()

    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)

    # Backward pass and optimization
    loss.backward()
    optimizer.step()

    # Print loss
    if (epoch + 1) % 1 == 0:
        _, predicted = torch.max(outputs, dim=1)
        accuracy = (predicted == y).float().mean().item() * 100
        print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {accuracy:.2f}%")

# Validation accuracy check
model.eval()
with torch.no_grad():
    outputs = model(X_val)
    _, predicted = torch.max(outputs, dim=1)
    val_accuracy = (predicted == y_val).float().mean().item() * 100

print(f"Validation Accuracy: {val_accuracy:.2f}%")

from google.colab import files
from PIL import Image
import torch.nn.functional as F
import torchvision.transforms as transforms
import torch
# Upload image
uploaded = files.upload()
# Get uploaded file path
image_path = list(uploaded.keys())[0]
# Load image and convert RGBA to RGB if needed
image = Image.open(image_path).convert("RGB")
# CLIP preprocessing (resize and normalize)
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.4815, 0.4578, 0.4082), std=(0.2686, 0.2613, 0.2758)),
])
# Move model to GPU (if available) and enforce float32
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device).float()
clip_model.to(device).float()
# Prepare image for CLIP model
image_input = preprocess(image).unsqueeze(0).to(device).float()
# Extract image features from CLIP
with torch.no_grad():
    image_features = clip_model.encode_image(image_input).float()
# Predict hatefulness using image features
model.eval()
with torch.no_grad():
    output = model(image_features).float()
    probability = F.softmax(output, dim=1)[:, 1].item()
print(f"Hatefulness Probability: {probability * 100:.2f}%")

optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

print("Sample Image Feature:", image_features[0][:5])
print("Sample Text Feature:", text_features[0][:5])
print("Combined Input Shape:", combined_input.shape)

with torch.no_grad():
    raw_output = model(combined_input)
    print("Raw Model Output:", raw_output)

def reset_weights(model):
    for layer in model.children():
        if hasattr(layer, 'reset_parameters'):
            layer.reset_parameters()

reset_weights(model)

import os

# Check the working directory and files
print("Current directory:", os.getcwd())
print("Files in directory:", os.listdir())

import matplotlib.pyplot as plt
import torch
import clip
from PIL import Image
import os

# Load CLIP model and preprocessing
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model, preprocess = clip.load("ViT-B/32", device=device)

# Ensure model and inputs use the same dtype
clip_model = clip_model.to(device).eval().half()  # Use half precision (16-bit)

# Visualize 9 feature maps from CLIP's vision model
def visualize_clip_feature_maps(model, image_path):
    # Ensure the image exists
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image not found: {image_path}")

    # Load and preprocess the image
    image = Image.open(image_path)
    image_input = preprocess(image).unsqueeze(0).to(device).half()  # Ensure half precision

    # Extract feature maps from the vision transformer (ViT)
    with torch.no_grad():
        vision_model = model.visual
        outputs = vision_model.conv1(image_input)  # First convolution layer output
        outputs = outputs.cpu().squeeze(0)  # Move to CPU for visualization

    print("Feature map shape:", outputs.shape)  # Shape like (64, 224, 224)

    # Visualize the first 9 feature maps (3x3 grid)
    plt.figure(figsize=(12, 12))
    for i in range(9):
        plt.subplot(3, 3, i + 1)
        plt.imshow(outputs[i].numpy(), cmap='viridis')
        plt.axis('off')
    plt.show()

# Use the available image
image_path = "/content/Example-of-hateful-meme.ppm"

# Call the visualization function
visualize_clip_feature_maps(clip_model, image_path)

print(model)

import os
import torch
import matplotlib.pyplot as plt
from PIL import Image

# Image folder path
image_folder = "/content"

# Select images for visualization
image_paths = [
    os.path.join(image_folder, "Example-of-hateful-meme.ppm"),
    os.path.join(image_folder, "mean_meme.png"),
]

# Check if files exist and display available images
print("Checking available images in the folder:")
print(os.listdir(image_folder))

for img_path in image_paths:
    if not os.path.isfile(img_path):
        print(f"Image not found: {img_path}")

# Function to extract image features using CLIP
def extract_clip_features(image_path, preprocess, clip_model):
    image = Image.open(image_path)
    image_input = preprocess(image).unsqueeze(0).to(device)

    with torch.no_grad():
        image_features = clip_model.encode_image(image_input)

    return image_features.to(torch.float32)  # Ensure dtype consistency

# Function to predict and visualize hatefulness
def visualize_predictions(model, image_paths, preprocess, clip_model):
    plt.figure(figsize=(15, 5))

    for i, img_path in enumerate(image_paths):
        # Ensure the image path exists
        if not os.path.isfile(img_path):
            print(f"Skipping missing image: {img_path}")
            continue

        # Extract image features using CLIP
        image_features = extract_clip_features(img_path, preprocess, clip_model)

        # Predict using the modified classifier
        with torch.no_grad():
            output = model(image_features)
            probability = output.softmax(dim=1)[:, 1].item() * 100

        # Display image with prediction
        image = Image.open(img_path)
        plt.subplot(1, len(image_paths), i + 1)
        plt.imshow(image)
        plt.title(f"Hatefulness: {probability:.2f}%")
        plt.axis("off")

    plt.show()

# Call the function with your model and images
visualize_predictions(model, image_paths, preprocess, clip_model)

